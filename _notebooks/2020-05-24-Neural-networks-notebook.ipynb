{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейронные сети в `.ipynb`\n",
    "> Попытка разобраться с вещами напрямую или косвенно связанными с нейронными сетями, чтобы самому стало несколько легче в понимании.\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter, theory]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL; DL\n",
    "1. Выбор функции ошибки:\n",
    "   1. Кратко;\n",
    "   2. Интуитивный подход;\n",
    "2. Устройство нейронных сетей;\n",
    "3. Поиск минимума функции ошибки (TODO: ссылка на градиентный спуск);\n",
    "4. Градиентный спуск в нейросетях (Backpropagation):\n",
    "   1. градиентный спуск;\n",
    "   2. вычислительные графы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка задачи\n",
    "Главной отличительной особенностью \"Обучение с учиталем\"(Superwised learning) является наличие обучающей выборки, состоящий из пар $x_i, y_i$, где $x_i$ - признаковое представление объекта, $y_i$ - целевая переменная.\n",
    "В этой области машинного обучения встают задачи классификации, регрессии и тд. \n",
    "\n",
    "Рассмотрим задачу регресии. `TODO: рассписать`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор функции ошибки\n",
    "### Интуитивный подход\n",
    "Задача регрессии, если упростить, сводится к тому, чтобы модель, описывающая \"скрытую в данных\" функциональную зависимость, не ошибалась. (Не учитываем погрешность при измерениях и переобучение)\n",
    "Модель не ошибается, когда спрогнозированные значения совпадают с реальными (или стремятся к ним).\n",
    "\n",
    "Как можно проверить, насколько модель ошибается?\n",
    "Нужно придумать подходящую [функцию ошибки](TODO).\n",
    "Например, мы можем измерять разницу между реальным ($y_i$) и прогнозным ($\\hat{y_i}$) значением для объекта из обучающей (или валидационной) выборки:\n",
    "\n",
    "$$y_i - \\hat{y_i}$$\n",
    "\n",
    "Но измерение ошибки модели в одной контрольной точке $i$ малоинформативно, возьмем сумму ошибок по всей обучаюшей выборке ($N$ - число наблюдений в выборке):\n",
    "$$\\sum_i^N{y_i - \\hat{y_i}}$$\n",
    "\n",
    "Вот тут мы и встречаем причину, почему просто **разность в качестве функции ошибки не подходит**:\n",
    "\n",
    "Ошибки модели на разных примерах из обучающей выборки могут иметь разные знаки, компенсируя друг друга при сложении.\n",
    "\n",
    "Пример, пусть существуют две модели A и B, которые имеют следующие ошибки $e_i = y_i - \\hat{y_i}$:\n",
    "\n",
    "| модель | $e_1$  | $e_2$  | $e_3$  | $e_4$  | $\\sum_i^N{e_i}$ |\n",
    "| :----: | :----: | :----: | :----: | :----: | :-------------: |\n",
    "|   A    | +3 000 | -1 000 | -1 000 | -1 000 |        0        |\n",
    "|   B    |   +1   |   +2   |   +1   |   +3   |        7        |\n",
    "\n",
    "Получается, что:\n",
    "> Для модели A суммарная ошибка будет равна 0, в то время как суммарная ошибка модели B будет равна 7.\n",
    "> \n",
    "> Исходя из этой логики модель A лучше, чем B.\n",
    "> \n",
    "> Hо это противоречит здравому смыслу, тк в каждой точке модель B лучше меньше ошибается, чем модель A. ¯\\\\\\_(ツ)_/¯\n",
    "\n",
    "Ключевая проблема в том, что **ошибки с разными знаками компенсируют друг друга**.\n",
    "Чтобы исправить этот недостаток, можно сделать такое преобразование над ошибками, которое уберет влияние знака.\n",
    "\n",
    "Ниже представленны примеры таких преобразований и формулы суммарной ошибки модели в результате этих преобразований:\n",
    "- $|x|$ (модуль числа):\n",
    "$$\\sum_i^N{\\lvert e_i \\rvert} = \\sum_i^N{\\lvert y_i - \\hat{y_i} \\rvert}$$\n",
    "- $x^2$ (квадрат числа):\n",
    "$$\\sum_i^N{e_i^2} = \\sum_i^N{(y_i - \\hat{y_i})^2}$$\n",
    "\n",
    "Рассмотрим уже известный пример с добавлением новых функций ошибки:\n",
    "\n",
    "| модель | $e_1$  | $e_2$  | $e_3$  | $e_4$  | $\\sum_i^N{e_i}$ | $\\sum_i^N{\\lvert e_i \\rvert}$ | $\\sum_i^N{e_i^2}$ |\n",
    "| :----: | :----: | :----: | :----: | :----: | :-------------: | :---------------------------: | :---------------: |\n",
    "|   A    | +3 000 | -1 000 | -1 000 | -1 000 |        0        |             6 000             |    12 000 000     |\n",
    "|   B    |   +1   |   +2   |   +1   |   +3   |        7        |               7               |        15         |\n",
    "\n",
    "\n",
    "Как можно видеть из таблицы, новые функции ошибки отражают следующий факт, что:\n",
    "> Модель A, которая на каждом наблюдении ошибалась сильнее, также сильнее ошибается в общем по всей выборке.\n",
    ">\n",
    "> Поэтому модель A хуже, чем модель B.\n",
    "\n",
    "Полезное свойство модуля и возведения в квадрат числа - **функции ошибки теперь ограничены снизу нулем**, те нуль мы получаем в случае, если модель не ошибается и всегда верно предсказывает значение целевой функции.\n",
    "\n",
    "Напоследок, предлагаю перейти от суммарных ошибок, к усредненным по наблюдениям, т.е.:\n",
    "- Средняя абсолютная ошибка или [MAE](https://en.wikipedia.org/wiki/Mean_absolute_error) (Mean absolute error):\n",
    "$$\\frac{1}{N} \\sum_i^N{\\lvert y_i - \\hat{y_i} \\rvert}$$\n",
    "- Среднеквадратичная ошибка или [MSE](https://en.wikipedia.org/wiki/Mean_squared_error) (Mean squared error):\n",
    "$$\\frac{1}{N} \\sum_i^N{(y_i - \\hat{y_i})^2}$$\n",
    "\n",
    "Пример для наглядности:\n",
    "\n",
    "| модель | $e_1$  | $e_2$  | $e_3$  | $e_4$  | $\\frac{1}{N}\\sum_i^N{e_i}$ |  MAE  |    MSE    |\n",
    "| :----: | :----: | :----: | :----: | :----: | :------------------------: | :---: | :-------: |\n",
    "|   A    | +3 000 | -1 000 | -1 000 | -1 000 |             0              | 1 500 | 4 000 000 |\n",
    "|   B    |   +1   |   +2   |   +1   |   +3   |            1.75            | 1.75  |   1.75    |\n",
    "\n",
    "Теперь функция ошибки отображает, как **в среднем ошибается модель**.\n",
    "\n",
    "Также размер значения усредненной функции ошибки не зависит напрямую от количества объектов в выборке. Это **удобно для сравнения полученных ошибок на обучающей, валидационной и тестовой выборках**. Ведь усреднение нивелирует рост значения функции ошибки из-за увеличения числа объектов, по которым ошибка расчитывается."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейросети\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск минимума функции ошибки\n",
    "\n",
    "Для начала, выберем функцию ошибки для нашей модели - пусть это будет MSE:\n",
    "$$L(\\theta) = \\frac{1}{N} \\sum_i^N{(y_i - \\hat{y_i})^2}$$\n",
    "\n",
    "TODO: почему MSE??? - оптимизация через град спуск\n",
    "\n",
    "Повторимся, в упрощенном и интуитивном понимании, для решения задачи регрессии:\n",
    "> Ищем такую модель, которая ошибается как можно реже, т.е. ее ошибка стремится к минимуму.\n",
    "\n",
    "Если выразить математически:\n",
    "$$L(\\theta) = \\frac{1}{N} \\sum_i^N{(y_i - \\hat{y_i})^2} \\rightarrow \\min_{\\theta}$$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}